# ğŸ”„ WORKFLOW: GeneraciÃ³n AutomÃ¡tica de Datos SintÃ©ticos

## ğŸ“‹ DescripciÃ³n General

Este workflow automatiza la **generaciÃ³n de datos sintÃ©ticos** para expandir tu dataset de entrenamiento. Clasifica automÃ¡ticamente las imÃ¡genes como **clean** o **degraded** y genera **10 variaciones sintÃ©ticas** para cada imagen subida.

## ğŸ¯ Objetivo Principal

**Automatizar la primera fase del proceso de generaciÃ³n de datos sintÃ©ticos:**

1. âœ… RecepciÃ³n automÃ¡tica de imÃ¡genes
2. âœ… ClasificaciÃ³n inteligente (clean vs degraded)
3. âœ… Almacenamiento en buckets apropiados
4. âœ… GeneraciÃ³n de 10 variaciones sintÃ©ticas
5. âœ… ActualizaciÃ³n automÃ¡tica de estadÃ­sticas

## ğŸ”„ Flujo del Workflow

### ğŸ“¥ **PASO 1: Webhook - Data Upload**

- **Endpoint**: `/webhook/data-generation`
- **MÃ©todo**: POST multipart/form-data
- **Input**: Imagen original (cualquier formato)

### ğŸ” **PASO 2: Classify Image Quality**

- **API**: `POST /classify/image-quality`
- **FunciÃ³n**: AnÃ¡lisis automÃ¡tico de calidad de imagen
- **MÃ©tricas**:
  - ğŸ” **Sharpness** (Varianza del Laplaciano)
  - ğŸ“Š **Gradient** (Magnitud promedio)
  - ğŸŒˆ **Contrast** (DesviaciÃ³n estÃ¡ndar)
  - ğŸ”§ **Noise Level** (AnÃ¡lisis gaussiano)
- **Output**: `"clean"` o `"degraded"` + mÃ©tricas

### ğŸ”€ **PASO 3: Check Classification**

- **DecisiÃ³n**: Rutear segÃºn clasificaciÃ³n
- **Branch A**: Si `classification == "clean"` â†’ Upload to Clean Bucket
- **Branch B**: Si `classification == "degraded"` â†’ Upload to Degraded Bucket

### â˜ï¸ **PASO 4A: Upload to Clean Bucket**

- **Destino**: `document-clean` bucket en MinIO
- **Siguiente**: Generar versiones degradadas

### â˜ï¸ **PASO 4B: Upload to Degraded Bucket**

- **Destino**: `document-degraded` bucket en MinIO
- **Siguiente**: Generar variaciones adicionales

### ğŸ¤– **PASO 5: Generate Synthetic Data**

- **API**: `POST /generate/synthetic-data`
- **ParÃ¡metros**:
  ```json
  {
    "source_bucket": "document-clean|document-degraded",
    "source_file": "filename.png",
    "target_count": 10,
    "generation_type": "degradation|variation",
    "output_bucket": "document-training"
  }
  ```

### â±ï¸ **PASO 6: Check Generation Status + Wait Loop**

- **API**: `GET /jobs/{job_id}`
- **Monitoreo**: Verifica cada 5 segundos
- **Estados**: `pending` â†’ `processing` â†’ `completed|failed`

### ğŸ“Š **PASO 7: Update Dataset Stats**

- **API**: `GET /dataset/stats?include_new=true`
- **FunciÃ³n**: Recalcula estadÃ­sticas del dataset

### ğŸ”” **PASO 8: Notify Completion**

- **API**: `POST /notify/dataset-updated`
- **FunciÃ³n**: Notifica expansiÃ³n del dataset
- **Output**: Log de eventos y mÃ©tricas

### âœ… **PASO 9: Response**

- **Success**: Retorna estadÃ­sticas de generaciÃ³n
- **Error**: Retorna detalles del error

## ğŸ¯ Tipos de GeneraciÃ³n SintÃ©tica

### ğŸ”§ **Degradation (desde imÃ¡genes clean)**

1. **Ruido Gaussiano**: Ïƒ = 5-15, probabilidad 80%
2. **Blur Gaussiano**: kernel 3x3, 5x5, 7x7, probabilidad 60%
3. **CompresiÃ³n JPEG**: quality 30-70, probabilidad 70%
4. **Ajuste Brillo/Contraste**: Î± = 0.7-1.3, Î² = Â±30, probabilidad 50%

### ğŸ”„ **Variation (desde imÃ¡genes degraded)**

1. **RotaciÃ³n Sutil**: Â±2 grados, probabilidad 40%
2. **Gamma Correction**: Î³ = 0.8-1.2, probabilidad 60%
3. **Salt-and-Pepper**: 2% pixels, probabilidad 30%
4. **Transformaciones GeomÃ©tricas**: Preservando degradaciÃ³n

## ğŸ“Š MÃ©tricas de ClasificaciÃ³n

### ğŸ” **Quality Score Calculation**

```python
quality_score = (
    laplacian_var * 0.4 +      # Sharpness weight
    avg_gradient * 0.3 +       # Edge definition
    contrast * 0.2 +           # Dynamic range
    (100 - noise_level) * 0.1  # Noise penalty
)

classification = "clean" if quality_score > 150 else "degraded"
```

### ğŸ“ˆ **Thresholds EmpÃ­ricos**

- **Clean**: Quality Score > 150
- **Degraded**: Quality Score â‰¤ 150
- **Confidence**: Normalizado a [0,1]

## ğŸŒ Endpoints API Nuevos

### `POST /classify/image-quality`

```python
# Input: Multipart file
# Output:
{
  "classification": "clean|degraded",
  "confidence": 0.85,
  "metrics": {
    "sharpness": 245.7,
    "gradient": 12.3,
    "contrast": 45.2,
    "noise": 8.1,
    "quality_score": 187.5
  },
  "filename": "document.png"
}
```

### `POST /generate/synthetic-data`

```python
# Input: JSON parameters
# Output:
{
  "job_id": "uuid-string",
  "status": "generation_started"
}
```

### `GET /dataset/stats`

```python
# Output:
{
  "buckets": {
    "document-clean": {"count": 15, "total_size_mb": 45.2},
    "document-degraded": {"count": 180, "total_size_mb": 234.7},
    "document-training": {"count": 1500, "total_size_mb": 890.3}
  },
  "total_samples": 1695,
  "timestamp": "2025-08-23T16:30:00"
}
```

## ğŸ§ª Testing del Workflow

### **MÃ©todo 1: n8n Webhook**

```bash
curl -X POST \
  -F "file=@document.png" \
  http://localhost:5678/webhook/data-generation
```

### **MÃ©todo 2: Cliente Python**

```python
from api.synthetic_data_client import SyntheticDataClient

client = SyntheticDataClient()
result = client.upload_for_synthetic_generation("test_image.png")
print(result)
```

### **MÃ©todo 3: API Directa**

```python
# 1. Clasificar
classification = client.classify_image_quality("image.png")

# 2. Generar sintÃ©ticos
job = client.start_synthetic_generation(
    source_bucket="document-clean",
    source_file="image.png",
    target_count=10
)

# 3. Monitorear
status = client.monitor_generation_job(job['job_id'])
```

## ğŸ“ˆ Beneficios del Workflow

âœ… **AutomatizaciÃ³n Completa**: Sin intervenciÃ³n manual  
âœ… **ClasificaciÃ³n Inteligente**: AnÃ¡lisis automÃ¡tico de calidad  
âœ… **ExpansiÃ³n Masiva**: 10x multiplicaciÃ³n del dataset  
âœ… **Almacenamiento Organizado**: Buckets separados por tipo  
âœ… **Monitoreo en Tiempo Real**: Estado de generaciÃ³n  
âœ… **Escalabilidad**: Procesamiento en background  
âœ… **IntegraciÃ³n Total**: Compatible con pipeline existente

## ğŸ”„ IntegraciÃ³n con Pipeline Principal

Este workflow **alimenta automÃ¡ticamente** tu pipeline de entrenamiento:

```
Imagen Original â†’ ClasificaciÃ³n â†’ GeneraciÃ³n SintÃ©tica â†’ Training Dataset â†’ Transfer Learning Gradual
```

Â¡Tu sistema ahora puede **expandir automÃ¡ticamente** el dataset con solo subir imÃ¡genes! ğŸš€
