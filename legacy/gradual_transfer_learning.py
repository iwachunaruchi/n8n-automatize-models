#!/usr/bin/env python3
"""
ImplementaciÃ³n de Transfer Learning Gradual para Restormer
Estrategia avanzada de fine-tuning por etapas
"""

import sys
import os
sys.path.append('src')

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import cv2
import numpy as np
import requests
from tqdm import tqdm
import matplotlib.pyplot as plt

# Importar nuestros modelos
from models.restormer import Restormer

class GradualTransferLearning:
    """
    Implementa Transfer Learning Gradual para Restormer
    
    ETAPAS:
    1. Congelar todo â†’ Entrenar solo clasificador final
    2. Descongelar Ãºltimas capas â†’ Entrenar refinement blocks  
    3. Descongelar capas medias â†’ Entrenar algunos transformer blocks
    4. Descongelar todo â†’ Fine-tuning completo con LR muy bajo
    """
    
    def __init__(self, model, device):
        self.model = model
        self.device = device
        self.stage = 0
        self.stage_configs = [
            {
                'name': 'ETAPA 1: Solo Refinement Blocks',
                'frozen_params': ['encoder', 'decoder', 'bottleneck'],
                'trainable_params': ['refinement'],
                'learning_rate': 1e-4,
                'epochs': 3,
                'description': 'Entrena solo los bloques de refinamiento final'
            },
            {
                'name': 'ETAPA 2: Decoder + Refinement',
                'frozen_params': ['encoder', 'bottleneck'],
                'trainable_params': ['decoder', 'refinement'],
                'learning_rate': 5e-5,
                'epochs': 3,
                'description': 'AÃ±ade el decoder manteniendo encoder congelado'
            },
            {
                'name': 'ETAPA 3: Bottleneck + Decoder + Refinement',
                'frozen_params': ['encoder'],
                'trainable_params': ['bottleneck', 'decoder', 'refinement'],
                'learning_rate': 2e-5,
                'epochs': 3,
                'description': 'AÃ±ade bottleneck, mantiene encoder congelado'
            },
            {
                'name': 'ETAPA 4: Fine-tuning Completo',
                'frozen_params': [],
                'trainable_params': ['encoder', 'bottleneck', 'decoder', 'refinement'],
                'learning_rate': 1e-5,
                'epochs': 5,
                'description': 'Entrena todo el modelo con LR ultra bajo'
            }
        ]
    
    def freeze_layers_by_name(self, frozen_components):
        """Congelar componentes especÃ­ficos del modelo"""
        
        total_params = 0
        frozen_params = 0
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            
            # Determinar si este parÃ¡metro debe estar congelado
            should_freeze = False
            for component in frozen_components:
                if component in name.lower():
                    should_freeze = True
                    break
            
            param.requires_grad = not should_freeze
            if should_freeze:
                frozen_params += param.numel()
        
        trainable_params = total_params - frozen_params
        print(f"   ðŸ§Š ParÃ¡metros congelados: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
        print(f"   ðŸ”¥ ParÃ¡metros entrenables: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
        
        return trainable_params
    
    def setup_stage(self, stage_idx):
        """Configurar una etapa especÃ­fica del transfer learning gradual"""
        
        if stage_idx >= len(self.stage_configs):
            print(f"âŒ Etapa {stage_idx} no existe")
            return None
        
        config = self.stage_configs[stage_idx]
        
        print(f"\nðŸŽ¯ {config['name']}")
        print("=" * 60)
        print(f"ðŸ“ {config['description']}")
        print(f"ðŸ”¥ Learning Rate: {config['learning_rate']}")
        print(f"ðŸ”„ Ã‰pocas: {config['epochs']}")
        
        # Configurar quÃ© parÃ¡metros entrenar
        trainable_params = self.freeze_layers_by_name(config['frozen_params'])
        
        # Configurar optimizador para esta etapa
        optimizer = optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=config['learning_rate'],
            weight_decay=1e-4
        )
        
        # Scheduler especÃ­fico para cada etapa
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config['epochs'], eta_min=config['learning_rate']/10
        )
        
        return {
            'optimizer': optimizer,
            'scheduler': scheduler,
            'config': config,
            'trainable_params': trainable_params
        }
    
    def print_model_structure(self):
        """Mostrar estructura del modelo para entender las capas"""
        
        print(f"\nðŸ” ESTRUCTURA DEL MODELO RESTORMER:")
        print("=" * 50)
        
        component_params = {}
        
        for name, param in self.model.named_parameters():
            # Clasificar parÃ¡metros por componente
            if 'encoder' in name.lower():
                component = 'Encoder'
            elif 'decoder' in name.lower():
                component = 'Decoder'
            elif 'bottleneck' in name.lower():
                component = 'Bottleneck'
            elif 'refinement' in name.lower():
                component = 'Refinement'
            else:
                component = 'Other'
            
            if component not in component_params:
                component_params[component] = 0
            component_params[component] += param.numel()
        
        total_params = sum(component_params.values())
        
        for component, params in component_params.items():
            percentage = params / total_params * 100
            print(f"ðŸ“Š {component}: {params:,} parÃ¡metros ({percentage:.1f}%)")

class AdvancedDocumentDataset(Dataset):
    """Dataset optimizado para transfer learning gradual"""
    
    def __init__(self, degraded_dir, clean_dir, img_size=128, augment=True):
        self.degraded_dir = degraded_dir
        self.clean_dir = clean_dir
        self.img_size = img_size
        self.augment = augment
        
        # Obtener pares vÃ¡lidos
        self.valid_pairs = []
        degraded_images = [f for f in os.listdir(degraded_dir) if f.endswith('.png')]
        clean_images = [f for f in os.listdir(clean_dir) if f.endswith('.png')]
        
        for deg_img in degraded_images:
            base_name = deg_img.replace('_deg_heavy', '').replace('_deg_medium', '').replace('_deg_light', '')
            base_name = base_name.replace('_var_1', '').replace('_var_2', '').replace('_var_3', '')
            
            for clean_img in clean_images:
                if base_name in clean_img or clean_img.replace('val_', '') == base_name:
                    self.valid_pairs.append((deg_img, clean_img))
                    break
        
        print(f"ðŸ“Š Dataset: {len(self.valid_pairs)} pares vÃ¡lidos")
    
    def __len__(self):
        return len(self.valid_pairs) * (4 if self.augment else 1)
    
    def __getitem__(self, idx):
        if self.augment:
            pair_idx = idx // 4
            aug_type = idx % 4
        else:
            pair_idx = idx
            aug_type = 0
        
        deg_name, clean_name = self.valid_pairs[pair_idx]
        
        # Cargar imÃ¡genes
        deg_path = os.path.join(self.degraded_dir, deg_name)
        clean_path = os.path.join(self.clean_dir, clean_name)
        
        degraded = cv2.imread(deg_path)
        clean = cv2.imread(clean_path)
        
        if degraded is None or clean is None:
            degraded = np.random.randint(0, 255, (self.img_size, self.img_size, 3), dtype=np.uint8)
            clean = degraded.copy()
        
        # Aplicar augmentation segÃºn tipo
        if aug_type == 1:
            degraded = cv2.flip(degraded, 1)
            clean = cv2.flip(clean, 1)
        elif aug_type == 2:
            degraded = cv2.flip(degraded, 0)
            clean = cv2.flip(clean, 0)
        elif aug_type == 3:
            degraded = cv2.rotate(degraded, cv2.ROTATE_180)
            clean = cv2.rotate(clean, cv2.ROTATE_180)
        
        # Procesamiento
        degraded = cv2.resize(degraded, (self.img_size, self.img_size))
        clean = cv2.resize(clean, (self.img_size, self.img_size))
        
        degraded = cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
        clean = cv2.cvtColor(clean, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
        
        degraded = torch.from_numpy(degraded).permute(2, 0, 1)
        clean = torch.from_numpy(clean).permute(2, 0, 1)
        
        return degraded, clean

def download_pretrained_model():
    """Descargar modelo preentrenado"""
    model_dir = "models/pretrained"
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, "restormer_denoising.pth")
    
    if os.path.exists(model_path):
        return model_path
    
    print("ðŸ“¥ Descargando modelo preentrenado...")
    url = "https://github.com/swz30/Restormer/releases/download/v1.0/gaussian_color_denoising_blind.pth"
    
    try:
        response = requests.get(url, stream=True)
        response.raise_for_status()
        total_size = int(response.headers.get('content-length', 0))
        
        with open(model_path, 'wb') as f:
            with tqdm(total=total_size, unit='B', unit_scale=True, desc="Descargando") as pbar:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        pbar.update(len(chunk))
        
        return model_path
    except Exception as e:
        print(f"âŒ Error: {e}")
        return None

def load_pretrained_model(device):
    """Cargar modelo preentrenado"""
    model_path = download_pretrained_model()
    if not model_path:
        return None
    
    model = Restormer(
        inp_channels=3, out_channels=3, dim=48,
        num_blocks=[4, 6, 6, 8], num_refinement_blocks=4,
        heads=[1, 2, 4, 8], ffn_expansion_factor=2.66,
        bias=False, LayerNorm_type='WithBias', dual_pixel_task=False
    )
    
    try:
        checkpoint = torch.load(model_path, map_location=device)
        if isinstance(checkpoint, dict):
            if 'params' in checkpoint:
                state_dict = checkpoint['params']
            elif 'model_state_dict' in checkpoint:
                state_dict = checkpoint['model_state_dict']
            elif 'state_dict' in checkpoint:
                state_dict = checkpoint['state_dict']
            else:
                state_dict = checkpoint
        else:
            state_dict = checkpoint
        
        model.load_state_dict(state_dict, strict=False)
        model = model.to(device)
        return model
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        model = model.to(device)
        return model

def gradual_transfer_learning():
    """Implementar transfer learning gradual completo"""
    
    print("ðŸš€ TRANSFER LEARNING GRADUAL")
    print("=" * 60)
    print("ðŸ“š CONCEPTO: Entrenar por etapas, desbloqueando progresivamente las capas")
    print("ðŸŽ¯ OBJETIVO: Preservar conocimiento preentrenado + adaptarse a tus datos")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nðŸ”§ Dispositivo: {device}")
    
    # Cargar modelo preentrenado
    model = load_pretrained_model(device)
    if model is None:
        return None
    
    # Crear gestor de transfer learning gradual
    gtl = GradualTransferLearning(model, device)
    
    # Mostrar estructura del modelo
    gtl.print_model_structure()
    
    # Dataset
    dataset = AdvancedDocumentDataset(
        degraded_dir="data/train/degraded",
        clean_dir="data/train/clean",
        img_size=128,
        augment=True
    )
    
    dataloader = DataLoader(
        dataset, batch_size=2, shuffle=True, num_workers=0
    )
    
    # Loss function
    criterion = nn.L1Loss()
    
    # Historial de entrenamiento
    training_history = {
        'stages': [],
        'losses': [],
        'learning_rates': []
    }
    
    print(f"\nðŸŽ­ INICIANDO TRANSFER LEARNING GRADUAL POR ETAPAS")
    print("=" * 60)
    
    # Ejecutar cada etapa
    for stage_idx in range(len(gtl.stage_configs)):
        
        # Configurar etapa
        stage_setup = gtl.setup_stage(stage_idx)
        if stage_setup is None:
            continue
        
        optimizer = stage_setup['optimizer']
        scheduler = stage_setup['scheduler']
        config = stage_setup['config']
        
        model.train()
        stage_losses = []
        
        # Entrenar esta etapa
        for epoch in range(config['epochs']):
            running_loss = 0.0
            num_batches = 0
            
            progress_bar = tqdm(dataloader, desc=f"Etapa {stage_idx+1} - Ã‰poca {epoch+1}/{config['epochs']}")
            
            for degraded, clean in progress_bar:
                degraded = degraded.to(device)
                clean = clean.to(device)
                
                optimizer.zero_grad()
                
                try:
                    restored = model(degraded)
                    loss = criterion(restored, clean)
                    
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(
                        filter(lambda p: p.requires_grad, model.parameters()), 
                        max_norm=0.5
                    )
                    optimizer.step()
                    
                    running_loss += loss.item()
                    num_batches += 1
                    
                    progress_bar.set_postfix({
                        'Loss': f'{loss.item():.6f}',
                        'Avg': f'{running_loss/num_batches:.6f}',
                        'LR': f'{optimizer.param_groups[0]["lr"]:.2e}'
                    })
                    
                except RuntimeError as e:
                    if "out of memory" in str(e):
                        print(f"âš ï¸ CUDA OOM en etapa {stage_idx+1}")
                        torch.cuda.empty_cache()
                        continue
                    else:
                        raise e
            
            # Calcular loss de la Ã©poca
            epoch_loss = running_loss / num_batches if num_batches > 0 else float('inf')
            stage_losses.append(epoch_loss)
            
            # Scheduler step
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            print(f"âœ… Etapa {stage_idx+1} - Ã‰poca {epoch+1}/{config['epochs']} - Loss: {epoch_loss:.6f} - LR: {current_lr:.2e}")
        
        # Guardar progreso de la etapa
        training_history['stages'].append(config['name'])
        training_history['losses'].append(stage_losses)
        training_history['learning_rates'].append([optimizer.param_groups[0]['lr']])
        
        # Guardar checkpoint de etapa
        stage_checkpoint_path = f"outputs/checkpoints/gradual_stage_{stage_idx+1}.pth"
        os.makedirs(os.path.dirname(stage_checkpoint_path), exist_ok=True)
        
        torch.save({
            'stage': stage_idx + 1,
            'model_state_dict': model.state_dict(),
            'stage_config': config,
            'stage_losses': stage_losses,
            'trainable_params': stage_setup['trainable_params']
        }, stage_checkpoint_path)
        
        print(f"ðŸ’¾ Checkpoint etapa {stage_idx+1} guardado")
        print(f"ðŸ“‰ Loss promedio etapa: {np.mean(stage_losses):.6f}")
    
    # Guardar modelo final
    final_model_path = "outputs/checkpoints/gradual_transfer_final.pth"
    torch.save({
        'model_state_dict': model.state_dict(),
        'training_history': training_history,
        'method': 'gradual_transfer_learning',
        'stages_completed': len(gtl.stage_configs)
    }, final_model_path)
    
    print(f"\nðŸŽ‰ TRANSFER LEARNING GRADUAL COMPLETADO!")
    print(f"ðŸ’¾ Modelo final: {final_model_path}")
    
    # Crear grÃ¡fico de progreso por etapas
    create_gradual_learning_plot(training_history)
    
    return model, final_model_path

def create_gradual_learning_plot(history):
    """Crear grÃ¡fico mostrando el progreso por etapas"""
    
    plt.figure(figsize=(15, 10))
    
    # Plot principal: Loss por etapa
    plt.subplot(2, 2, 1)
    all_losses = []
    stage_boundaries = []
    current_epoch = 0
    
    colors = ['red', 'orange', 'green', 'blue', 'purple']
    
    for i, (stage_name, stage_losses) in enumerate(zip(history['stages'], history['losses'])):
        epochs = list(range(current_epoch, current_epoch + len(stage_losses)))
        plt.plot(epochs, stage_losses, 'o-', color=colors[i % len(colors)], 
                label=f'Etapa {i+1}', linewidth=2, markersize=4)
        
        all_losses.extend(stage_losses)
        stage_boundaries.append(current_epoch + len(stage_losses))
        current_epoch += len(stage_losses)
    
    # LÃ­neas verticales para separar etapas
    for boundary in stage_boundaries[:-1]:
        plt.axvline(x=boundary-0.5, color='gray', linestyle='--', alpha=0.7)
    
    plt.title('Transfer Learning Gradual: PÃ©rdida por Etapa')
    plt.xlabel('Ã‰poca Global')
    plt.ylabel('Loss (L1)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: ComparaciÃ³n promedio por etapa
    plt.subplot(2, 2, 2)
    stage_avg_losses = [np.mean(losses) for losses in history['losses']]
    stage_names = [f'Etapa {i+1}' for i in range(len(stage_avg_losses))]
    
    bars = plt.bar(stage_names, stage_avg_losses, color=colors[:len(stage_avg_losses)])
    plt.title('Loss Promedio por Etapa')
    plt.ylabel('Loss Promedio')
    plt.xticks(rotation=45)
    
    # AÃ±adir valores en las barras
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.4f}', ha='center', va='bottom')
    
    # Subplot 3: DescripciÃ³n del mÃ©todo
    plt.subplot(2, 2, 3)
    plt.text(0.1, 0.9, "ðŸŽ¯ TRANSFER LEARNING GRADUAL", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)
    plt.text(0.1, 0.8, "Estrategia por etapas:", fontsize=12, transform=plt.gca().transAxes)
    
    descriptions = [
        "Etapa 1: Solo refinement blocks",
        "Etapa 2: + Decoder",
        "Etapa 3: + Bottleneck", 
        "Etapa 4: Modelo completo"
    ]
    
    for i, desc in enumerate(descriptions):
        plt.text(0.1, 0.7 - i*0.1, f"{i+1}. {desc}", fontsize=10, transform=plt.gca().transAxes)
    
    plt.text(0.1, 0.2, "âœ… Preserva conocimiento preentrenado", fontsize=10, color='green', transform=plt.gca().transAxes)
    plt.text(0.1, 0.1, "âœ… AdaptaciÃ³n gradual a tus datos", fontsize=10, color='green', transform=plt.gca().transAxes)
    
    plt.axis('off')
    
    # Subplot 4: MÃ©tricas finales
    plt.subplot(2, 2, 4)
    final_loss = history['losses'][-1][-1] if history['losses'] else 0
    total_epochs = sum(len(losses) for losses in history['losses'])
    
    plt.text(0.1, 0.8, "ðŸ“Š MÃ‰TRICAS FINALES", fontsize=14, fontweight='bold', transform=plt.gca().transAxes)
    plt.text(0.1, 0.6, f"Loss final: {final_loss:.6f}", fontsize=12, transform=plt.gca().transAxes)
    plt.text(0.1, 0.5, f"Ã‰pocas totales: {total_epochs}", fontsize=12, transform=plt.gca().transAxes)
    plt.text(0.1, 0.4, f"Etapas completadas: {len(history['stages'])}", fontsize=12, transform=plt.gca().transAxes)
    
    plt.text(0.1, 0.2, "ðŸŽ¯ Ventajas del mÃ©todo gradual:", fontsize=12, fontweight='bold', transform=plt.gca().transAxes)
    plt.text(0.1, 0.1, "â€¢ Estabilidad en entrenamiento", fontsize=10, transform=plt.gca().transAxes)
    plt.text(0.1, 0.05, "â€¢ PreservaciÃ³n de caracterÃ­sticas", fontsize=10, transform=plt.gca().transAxes)
    
    plt.axis('off')
    
    plt.tight_layout()
    
    plot_path = "outputs/analysis/gradual_transfer_learning.png"
    os.makedirs(os.path.dirname(plot_path), exist_ok=True)
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"ðŸ“Š AnÃ¡lisis gradual guardado: {plot_path}")

if __name__ == "__main__":
    print("ðŸŽ“ TRANSFER LEARNING GRADUAL")
    print("=" * 50)
    print("ðŸ“š CONCEPTO:")
    print("   â€¢ Entrenar por etapas progresivas")
    print("   â€¢ Desbloquear capas gradualmente") 
    print("   â€¢ Preservar conocimiento preentrenado")
    print("   â€¢ Adaptarse suavemente a datos especÃ­ficos")
    print()
    print("ðŸ”„ PROCESO:")
    print("   1. Congelar encoder â†’ Entrenar solo refinement")
    print("   2. AÃ±adir decoder â†’ Mantener encoder congelado")
    print("   3. AÃ±adir bottleneck â†’ Encoder aÃºn congelado")
    print("   4. Desbloquear todo â†’ Fine-tuning completo")
    
    model, model_path = gradual_transfer_learning()
    
    if model and model_path:
        print(f"\nðŸŽ¯ SIGUIENTE PASO:")
        print(f"   Ejecuta: python test_gradual_model.py")
        print(f"   Para comparar transfer learning gradual vs otros mÃ©todos")
