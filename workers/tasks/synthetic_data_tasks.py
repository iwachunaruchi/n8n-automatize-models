#!/usr/bin/env python3
"""
üé® SYNTHETIC DATA TASKS - RQ SPECIALIZED
=======================================
Tasks especializadas para generaci√≥n de datos sint√©ticos usando RQ.
"""

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

# Importar utilidades RQ
from ..utils.rq_utils import RQJobProgressTracker, setup_job_environment

logger = logging.getLogger(__name__)

def generate_synthetic_data_job(clean_bucket: str,
                               requested_count: int,
                               job_type: str = "training_pairs_generation",
                               **kwargs) -> Dict[str, Any]:
    """
    üé® Job de generaci√≥n de datos sint√©ticos
    
    Args:
        clean_bucket: Bucket con im√°genes limpias
        requested_count: N√∫mero de pares a generar
        job_type: Tipo de trabajo
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la generaci√≥n
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üé® Iniciando generaci√≥n de datos sint√©ticos: {requested_count} pares desde {clean_bucket}")
    
    try:
        tracker.update_progress(5, "Preparando generaci√≥n de datos sint√©ticos...")
        
        # Importar servicio de datos sint√©ticos
        from api.services.synthetic_data_service import synthetic_data_service
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando generaci√≥n...")
        
        # Ejecutar generaci√≥n de datos sint√©ticos con los par√°metros correctos
        generation_result = synthetic_data_service.generate_training_pairs(
            clean_bucket=clean_bucket,
            count=requested_count
        )
        
        tracker.update_progress(95, "Finalizando y guardando metadatos...")
        
        # Preparar resultado final
        final_result = {
            'status': 'completed',
            'generation_result': generation_result,
            'clean_bucket': clean_bucket,
            'requested_count': requested_count,
            'completed_at': datetime.now().isoformat(),
            'job_type': job_type,
            'generated_pairs': requested_count
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Generaci√≥n de {requested_count} pares completada")
        
        logger.info(f"‚úÖ Generaci√≥n de datos sint√©ticos completada: {requested_count} pares")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Synthetic Data Generation")
        logger.error(f"‚ùå Error en generaci√≥n de datos sint√©ticos: {e}")
        raise

def augment_dataset_job(bucket: str,
                       target_count: int,
                       job_type: str = "dataset_augmentation",
                       **kwargs) -> Dict[str, Any]:
    """
    üìà Job de aumento de dataset
    
    Args:
        bucket: Bucket con im√°genes a aumentar
        target_count: N√∫mero objetivo de im√°genes
        job_type: Tipo de trabajo
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados del aumento
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üìà Iniciando aumento de dataset: objetivo {target_count} im√°genes desde {bucket}")
    
    try:
        tracker.update_progress(5, "Preparando aumento de dataset...")
        
        # Importar servicio de datos sint√©ticos
        from api.services.synthetic_data_service import synthetic_data_service
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando augmentaci√≥n...")
        
        # Ejecutar augmentaci√≥n usando el servicio
        augmentation_result = synthetic_data_service.augment_dataset(
            bucket=bucket,
            target_count=target_count
        )
        
        tracker.update_progress(95, "Finalizando augmentaci√≥n...")
        
        final_result = {
            'status': 'completed',
            'augmentation_result': augmentation_result,
            'bucket': bucket,
            'target_count': target_count,
            'completed_at': datetime.now().isoformat(),
            'job_type': job_type
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Aumento de dataset completado: {target_count} objetivo")
        
        logger.info(f"‚úÖ Aumento de dataset completado para bucket {bucket}")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Dataset Augmentation")
        logger.error(f"‚ùå Error en aumento de dataset: {e}")
        raise

def validate_synthetic_data_job(bucket_name: str = "document-training",
                               sample_size: int = 50,
                               **kwargs) -> Dict[str, Any]:
    """
    ‚úÖ Job de validaci√≥n de datos sint√©ticos
    
    Args:
        bucket_name: Bucket con datos sint√©ticos
        sample_size: Tama√±o de muestra para validaci√≥n
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la validaci√≥n
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"‚úÖ Iniciando validaci√≥n de datos sint√©ticos en {bucket_name}")
    
    try:
        tracker.update_progress(5, "Preparando validaci√≥n...")
        
        # Importar servicios
        from api.services.minio_service import minio_service
        from api.services.image_analysis_service import image_analysis_service
        
        # Obtener muestra de im√°genes
        tracker.update_progress(10, "Obteniendo muestra de datos...")
        all_files = minio_service.list_files(bucket_name)
        
        import random
        sample_files = random.sample(all_files, min(sample_size, len(all_files)))
        
        validation_results = {
            'total_files': len(all_files),
            'sample_size': len(sample_files),
            'valid_images': 0,
            'invalid_images': 0,
            'quality_scores': [],
            'format_issues': [],
            'size_issues': []
        }
        
        # Validar cada imagen de la muestra
        for i, file_name in enumerate(sample_files):
            progress = int(10 + ((i / len(sample_files)) * 80))
            tracker.update_progress(progress, f"Validando {file_name}")
            
            try:
                # Analizar imagen
                analysis_result = image_analysis_service.analyze_image_quality(
                    bucket_name, file_name
                )
                
                if analysis_result['is_valid']:
                    validation_results['valid_images'] += 1
                    validation_results['quality_scores'].append(analysis_result['quality_score'])
                else:
                    validation_results['invalid_images'] += 1
                    if 'format_error' in analysis_result:
                        validation_results['format_issues'].append(file_name)
                    if 'size_error' in analysis_result:
                        validation_results['size_issues'].append(file_name)
                        
            except Exception as img_error:
                logger.warning(f"Error validando {file_name}: {img_error}")
                validation_results['invalid_images'] += 1
        
        # Calcular estad√≠sticas finales
        validation_results['average_quality'] = (
            sum(validation_results['quality_scores']) / len(validation_results['quality_scores'])
            if validation_results['quality_scores'] else 0
        )
        validation_results['validation_rate'] = (
            validation_results['valid_images'] / len(sample_files) * 100
            if sample_files else 0
        )
        
        final_result = {
            'status': 'completed',
            'validation_results': validation_results,
            'bucket_name': bucket_name,
            'sample_size': sample_size,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'synthetic_data_validation'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Validaci√≥n completada: {validation_results['validation_rate']:.1f}% v√°lidas")
        
        logger.info(f"‚úÖ Validaci√≥n completada: {validation_results['validation_rate']:.1f}% im√°genes v√°lidas")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Synthetic Data Validation")
        logger.error(f"‚ùå Error en validaci√≥n de datos sint√©ticos: {e}")
        raise

def generate_nafnet_dataset_job(source_bucket: str,
                               count: int,
                               task: str = "SIDD-width64",
                               train_val_split: bool = True,
                               **kwargs) -> Dict[str, Any]:
    """
    üéØ Job de generaci√≥n de dataset NAFNet estructurado
    
    Args:
        source_bucket: Bucket con im√°genes fuente
        count: N√∫mero total de pares a generar
        task: Tarea NAFNet espec√≠fica
        train_val_split: Si dividir en train/val
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la generaci√≥n NAFNet
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üéØ Iniciando generaci√≥n de dataset NAFNet: {count} pares para tarea '{task}'")
    
    try:
        tracker.update_progress(5, f"Preparando generaci√≥n dataset NAFNet para {task}...")
        
        # Importar servicio de datos sint√©ticos
        from api.services.synthetic_data_service import synthetic_data_service
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando generaci√≥n NAFNet...")
        
        # Ejecutar generaci√≥n de dataset NAFNet estructurado
        generation_result = synthetic_data_service.generate_nafnet_training_dataset(
            source_bucket=source_bucket,
            count=count,
            task=task,
            train_val_split=train_val_split
        )
        
        tracker.update_progress(95, "Finalizando generaci√≥n NAFNet...")
        
        # Preparar resultado final
        final_result = {
            'status': 'completed',
            'generation_result': generation_result,
            'source_bucket': source_bucket,
            'count': count,
            'task': task,
            'train_val_split': train_val_split,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'nafnet_dataset_generation',
            'dataset_structure': generation_result.get('dataset_structure', 'N/A'),
            'total_generated': generation_result.get('total_generated', 0)
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Dataset NAFNet generado: {generation_result.get('total_generated', 0)} pares")
        
        logger.info(f"‚úÖ Dataset NAFNet '{task}' completado: {generation_result.get('total_generated', 0)} pares")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "NAFNet Dataset Generation")
        logger.error(f"‚ùå Error en generaci√≥n dataset NAFNet: {e}")
        raise

def validate_nafnet_dataset_job(task: str = "SIDD-width64",
                               **kwargs) -> Dict[str, Any]:
    """
    ‚úÖ Job de validaci√≥n de dataset NAFNet
    
    Args:
        task: Tarea NAFNet a validar
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la validaci√≥n NAFNet
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"‚úÖ Iniciando validaci√≥n de dataset NAFNet para tarea '{task}'")
    
    try:
        tracker.update_progress(5, f"Preparando validaci√≥n NAFNet para {task}...")
        
        # Importar servicios
        from api.services.synthetic_data_service import synthetic_data_service
        from api.services.minio_service import minio_service
        
        tracker.update_progress(10, "Obteniendo informaci√≥n del dataset...")
        
        # Obtener informaci√≥n del dataset NAFNet
        dataset_info = synthetic_data_service.get_nafnet_dataset_info(task)
        
        if dataset_info["status"] != "success":
            raise Exception(f"Error obteniendo info dataset: {dataset_info['message']}")
        
        dataset_stats = dataset_info["dataset_info"]
        
        tracker.update_progress(30, "Validando estructura de directorios...")
        
        # Validar estructura completa
        validation_results = {
            'task': task,
            'structure_valid': True,
            'train_pairs': dataset_stats['structure']['complete_pairs']['train'],
            'val_pairs': dataset_stats['structure']['complete_pairs']['val'],
            'total_pairs': dataset_stats['total_pairs'],
            'issues': [],
            'recommendations': []
        }
        
        # Verificar que hay pares suficientes
        if validation_results['train_pairs'] < 10:
            validation_results['issues'].append("Muy pocos pares de entrenamiento (< 10)")
            validation_results['recommendations'].append("Generar m√°s pares de entrenamiento")
        
        if validation_results['val_pairs'] < 2:
            validation_results['issues'].append("Muy pocos pares de validaci√≥n (< 2)")
            validation_results['recommendations'].append("Generar m√°s pares de validaci√≥n")
        
        # Verificar balance train/val
        total_pairs = validation_results['train_pairs'] + validation_results['val_pairs']
        if total_pairs > 0:
            val_ratio = validation_results['val_pairs'] / total_pairs
            if val_ratio < 0.1 or val_ratio > 0.3:
                validation_results['issues'].append(f"Ratio de validaci√≥n no √≥ptimo: {val_ratio:.2f}")
                validation_results['recommendations'].append("Ratio recomendado: 0.15-0.25")
        
        tracker.update_progress(80, "Validando integridad de archivos...")
        
        # Aqu√≠ se pueden agregar m√°s validaciones espec√≠ficas
        # Por ejemplo, verificar que las im√°genes lq y gt coinciden en n√∫mero
        
        validation_results['structure_valid'] = len(validation_results['issues']) == 0
        validation_results['health_score'] = max(0, 100 - len(validation_results['issues']) * 20)
        
        final_result = {
            'status': 'completed',
            'validation_results': validation_results,
            'task': task,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'nafnet_dataset_validation'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Validaci√≥n NAFNet completada: {validation_results['health_score']}% saludable")
        
        logger.info(f"‚úÖ Validaci√≥n NAFNet '{task}' completada: {validation_results['health_score']}% saludable")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "NAFNet Dataset Validation")
        logger.error(f"‚ùå Error en validaci√≥n NAFNet: {e}")
        raise
