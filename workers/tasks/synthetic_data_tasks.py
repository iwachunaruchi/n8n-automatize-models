#!/usr/bin/env python3
"""
üé® SYNTHETIC DATA TASKS - RQ SPECIALIZED
=======================================
Tasks especializadas para generaci√≥n de datos sint√©ticos usando RQ.
"""

import logging
from typing import Dict, Any, Optional, List
from datetime import datetime

# Importar utilidades RQ
from ..utils.rq_utils import RQJobProgressTracker, setup_job_environment

logger = logging.getLogger(__name__)

def generate_synthetic_data_job(clean_bucket: str,
                               requested_count: int,
                               job_type: str = "training_pairs_generation",
                               **kwargs) -> Dict[str, Any]:
    """
    üé® Job de generaci√≥n de datos sint√©ticos
    
    Args:
        clean_bucket: Bucket con im√°genes limpias
        requested_count: N√∫mero de pares a generar
        job_type: Tipo de trabajo
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la generaci√≥n
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üé® Iniciando generaci√≥n de datos sint√©ticos: {requested_count} pares desde {clean_bucket}")
    
    try:
        tracker.update_progress(5, "Preparando generaci√≥n de datos sint√©ticos...")
        
        # Importar servicio de datos sint√©ticos
        from api.services.synthetic_data_service import synthetic_data_service
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando generaci√≥n...")
        
        # Ejecutar generaci√≥n de datos sint√©ticos con los par√°metros correctos
        generation_result = synthetic_data_service.generate_training_pairs(
            clean_bucket=clean_bucket,
            count=requested_count
        )
        
        tracker.update_progress(95, "Finalizando y guardando metadatos...")
        
        # Preparar resultado final
        final_result = {
            'status': 'completed',
            'generation_result': generation_result,
            'clean_bucket': clean_bucket,
            'requested_count': requested_count,
            'completed_at': datetime.now().isoformat(),
            'job_type': job_type,
            'generated_pairs': requested_count
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Generaci√≥n de {requested_count} pares completada")
        
        logger.info(f"‚úÖ Generaci√≥n de datos sint√©ticos completada: {requested_count} pares")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Synthetic Data Generation")
        logger.error(f"‚ùå Error en generaci√≥n de datos sint√©ticos: {e}")
        raise

def augment_dataset_job(bucket: str,
                       target_count: int,
                       job_type: str = "dataset_augmentation",
                       **kwargs) -> Dict[str, Any]:
    """
    üìà Job de aumento de dataset
    
    Args:
        bucket: Bucket con im√°genes a aumentar
        target_count: N√∫mero objetivo de im√°genes
        job_type: Tipo de trabajo
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados del aumento
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üìà Iniciando aumento de dataset: objetivo {target_count} im√°genes desde {bucket}")
    
    try:
        tracker.update_progress(5, "Preparando aumento de dataset...")
        
        # Importar servicio de datos sint√©ticos
        from api.services.synthetic_data_service import synthetic_data_service
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando augmentaci√≥n...")
        
        # Ejecutar augmentaci√≥n usando el servicio
        augmentation_result = synthetic_data_service.augment_dataset(
            bucket=bucket,
            target_count=target_count
        )
        
        tracker.update_progress(95, "Finalizando augmentaci√≥n...")
        
        final_result = {
            'status': 'completed',
            'augmentation_result': augmentation_result,
            'bucket': bucket,
            'target_count': target_count,
            'completed_at': datetime.now().isoformat(),
            'job_type': job_type
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Aumento de dataset completado: {target_count} objetivo")
        
        logger.info(f"‚úÖ Aumento de dataset completado para bucket {bucket}")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Dataset Augmentation")
        logger.error(f"‚ùå Error en aumento de dataset: {e}")
        raise

def validate_synthetic_data_job(bucket_name: str = "document-training",
                               sample_size: int = 50,
                               **kwargs) -> Dict[str, Any]:
    """
    ‚úÖ Job de validaci√≥n de datos sint√©ticos
    
    Args:
        bucket_name: Bucket con datos sint√©ticos
        sample_size: Tama√±o de muestra para validaci√≥n
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados de la validaci√≥n
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"‚úÖ Iniciando validaci√≥n de datos sint√©ticos en {bucket_name}")
    
    try:
        tracker.update_progress(5, "Preparando validaci√≥n...")
        
        # Importar servicios
        from api.services.minio_service import minio_service
        from api.services.image_analysis_service import image_analysis_service
        
        # Obtener muestra de im√°genes
        tracker.update_progress(10, "Obteniendo muestra de datos...")
        all_files = minio_service.list_files(bucket_name)
        
        import random
        sample_files = random.sample(all_files, min(sample_size, len(all_files)))
        
        validation_results = {
            'total_files': len(all_files),
            'sample_size': len(sample_files),
            'valid_images': 0,
            'invalid_images': 0,
            'quality_scores': [],
            'format_issues': [],
            'size_issues': []
        }
        
        # Validar cada imagen de la muestra
        for i, file_name in enumerate(sample_files):
            progress = int(10 + ((i / len(sample_files)) * 80))
            tracker.update_progress(progress, f"Validando {file_name}")
            
            try:
                # Analizar imagen
                analysis_result = image_analysis_service.analyze_image_quality(
                    bucket_name, file_name
                )
                
                if analysis_result['is_valid']:
                    validation_results['valid_images'] += 1
                    validation_results['quality_scores'].append(analysis_result['quality_score'])
                else:
                    validation_results['invalid_images'] += 1
                    if 'format_error' in analysis_result:
                        validation_results['format_issues'].append(file_name)
                    if 'size_error' in analysis_result:
                        validation_results['size_issues'].append(file_name)
                        
            except Exception as img_error:
                logger.warning(f"Error validando {file_name}: {img_error}")
                validation_results['invalid_images'] += 1
        
        # Calcular estad√≠sticas finales
        validation_results['average_quality'] = (
            sum(validation_results['quality_scores']) / len(validation_results['quality_scores'])
            if validation_results['quality_scores'] else 0
        )
        validation_results['validation_rate'] = (
            validation_results['valid_images'] / len(sample_files) * 100
            if sample_files else 0
        )
        
        final_result = {
            'status': 'completed',
            'validation_results': validation_results,
            'bucket_name': bucket_name,
            'sample_size': sample_size,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'synthetic_data_validation'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Validaci√≥n completada: {validation_results['validation_rate']:.1f}% v√°lidas")
        
        logger.info(f"‚úÖ Validaci√≥n completada: {validation_results['validation_rate']:.1f}% im√°genes v√°lidas")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Synthetic Data Validation")
        logger.error(f"‚ùå Error en validaci√≥n de datos sint√©ticos: {e}")
        raise
