#!/usr/bin/env python3
"""
üß† TRAINING TASKS - RQ SPECIALIZED
==================================
Tasks especializadas para entrenamiento de modelos usando RQ.
"""

import logging
from typing import Dict, Any, Optional
from datetime import datetime

# Importar utilidades RQ
from ..utils.rq_utils import RQJobProgressTracker, setup_job_environment, execute_with_progress

logger = logging.getLogger(__name__)

def layer2_training_job(num_epochs: int = 10, 
                       batch_size: int = 8, 
                       max_pairs: int = 1000,
                       use_training_bucket: bool = True,
                       **kwargs) -> Dict[str, Any]:
    """
    üß† Job de entrenamiento Layer 2
    
    Args:
        num_epochs: N√∫mero de √©pocas de entrenamiento
        batch_size: Tama√±o del batch
        max_pairs: M√°ximo n√∫mero de pares de entrenamiento
        use_training_bucket: Usar bucket de entrenamiento
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados del entrenamiento
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üß† Iniciando Layer 2 training: {num_epochs} √©pocas, batch_size: {batch_size}")
    
    try:
        # Importar servicios necesarios
        tracker.update_progress(5, "Cargando servicios de entrenamiento...")
        from api.services.training_service import training_service
        
        # Configurar par√°metros de entrenamiento
        training_params = {
            'num_epochs': num_epochs,
            'batch_size': batch_size,
            'max_pairs': max_pairs,
            'use_training_bucket': use_training_bucket
        }
        
        tracker.update_progress(10, "Configuraci√≥n completada, iniciando entrenamiento...")
        
        # Callback de progreso para el entrenamiento
        def training_progress_callback(progress: float, message: str):
            # Mapear progreso del entrenamiento (10-90% del job total)
            job_progress = int(10 + (progress * 0.8))
            tracker.update_progress(job_progress, f"Entrenamiento: {message}")
        
        # Ejecutar entrenamiento
        training_result = training_service.train_layer2(
            num_epochs=num_epochs,
            batch_size=batch_size,
            max_pairs=max_pairs,
            use_training_bucket=use_training_bucket,
            progress_callback=training_progress_callback
        )
        
        tracker.update_progress(95, "Finalizando y guardando resultados...")
        
        # Preparar resultado final
        final_result = {
            'status': 'completed',
            'training_result': training_result,
            'parameters': training_params,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'layer2_training'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, "Entrenamiento Layer 2 completado exitosamente")
        
        logger.info(f"‚úÖ Layer 2 training completado: {final_result['status']}")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Layer 2 Training")
        logger.error(f"‚ùå Error en Layer 2 training: {e}")
        raise

def layer1_training_job(model_type: str = "restormer",
                       num_epochs: int = 20,
                       batch_size: int = 4,
                       **kwargs) -> Dict[str, Any]:
    """
    üîß Job de entrenamiento Layer 1
    
    Args:
        model_type: Tipo de modelo (restormer, nafnet, etc.)
        num_epochs: N√∫mero de √©pocas
        batch_size: Tama√±o del batch
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados del entrenamiento
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üîß Iniciando Layer 1 training: {model_type}, {num_epochs} √©pocas")
    
    try:
        tracker.update_progress(5, "Preparando entrenamiento Layer 1...")
        
        # Importar m√≥dulo de entrenamiento Layer 1
        from layers.train_layers.train_layer_1 import train_layer_1
        
        training_params = {
            'model_type': model_type,
            'num_epochs': num_epochs,
            'batch_size': batch_size
        }
        
        tracker.update_progress(10, f"Iniciando entrenamiento {model_type}...")
        
        # Callback de progreso
        def l1_progress_callback(epoch: int, total_epochs: int, loss: float):
            progress = int(10 + ((epoch / total_epochs) * 80))
            tracker.update_progress(progress, f"√âpoca {epoch}/{total_epochs}, Loss: {loss:.4f}")
        
        # Ejecutar entrenamiento Layer 1
        training_result = train_layer_1(
            model_type=model_type,
            num_epochs=num_epochs,
            batch_size=batch_size,
            progress_callback=l1_progress_callback
        )
        
        final_result = {
            'status': 'completed',
            'training_result': training_result,
            'parameters': training_params,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'layer1_training'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, f"Entrenamiento Layer 1 ({model_type}) completado")
        
        logger.info(f"‚úÖ Layer 1 training completado: {model_type}")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Layer 1 Training")
        logger.error(f"‚ùå Error en Layer 1 training: {e}")
        raise

def fine_tuning_job(base_model: str,
                   dataset_path: str,
                   learning_rate: float = 1e-4,
                   num_epochs: int = 15,
                   **kwargs) -> Dict[str, Any]:
    """
    üéØ Job de fine-tuning de modelos
    
    Args:
        base_model: Modelo base para fine-tuning
        dataset_path: Ruta del dataset
        learning_rate: Tasa de aprendizaje
        num_epochs: N√∫mero de √©pocas
        **kwargs: Par√°metros adicionales de RQ
    
    Returns:
        Dict con resultados del fine-tuning
    """
    setup_job_environment()
    tracker = RQJobProgressTracker()
    
    logger.info(f"üéØ Iniciando Fine-tuning: {base_model}")
    
    try:
        tracker.update_progress(5, "Preparando fine-tuning...")
        
        # Simular fine-tuning (implementar l√≥gica real aqu√≠)
        for epoch in range(num_epochs):
            progress = int(10 + ((epoch / num_epochs) * 85))
            tracker.update_progress(progress, f"Fine-tuning √©poca {epoch+1}/{num_epochs}")
            
            # Simular tiempo de entrenamiento
            import time
            time.sleep(2)
        
        final_result = {
            'status': 'completed',
            'base_model': base_model,
            'dataset_path': dataset_path,
            'learning_rate': learning_rate,
            'num_epochs': num_epochs,
            'completed_at': datetime.now().isoformat(),
            'job_type': 'fine_tuning'
        }
        
        tracker.set_result(final_result)
        tracker.update_progress(100, "Fine-tuning completado")
        
        logger.info(f"‚úÖ Fine-tuning completado: {base_model}")
        return final_result
        
    except Exception as e:
        tracker.log_error(e, "Fine-tuning")
        logger.error(f"‚ùå Error en fine-tuning: {e}")
        raise
